{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "informal-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "designing-aluminum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetData():\n",
    "\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "\n",
    "#     def ergodic(self):\n",
    "#         return self.getCorpus(self, self.root)\n",
    "\n",
    "    def getCorpus(self):\n",
    "        corpus = []\n",
    "        r1 = u'[a-zA-Z0-9’!\"#$%&\\'()*+,-./:：;<=>?@，。?★、…【】《》？“”‘’！[\\\\]^_`{|}~]+'  # 过滤字符\n",
    "        listdir = os.listdir(self.root)\n",
    "        count=0\n",
    "        for file in listdir:\n",
    "            path  = os.path.join(self.root, file)\n",
    "            if os.path.isfile(path):\n",
    "                with open(os.path.abspath(path), \"r\", encoding='gb18030') as file:\n",
    "                    filecontext = file.read();\n",
    "                    filecontext = re.sub(r1, '', filecontext)\n",
    "                    filecontext = filecontext.replace(\"\\n\", '')\n",
    "                    filecontext = filecontext.replace(\" \", '')\n",
    "                    filecontext = filecontext.replace(\"本书来自www.cr173.com免费txt小说下载站\\n更多更新免费电子书请关注www.cr173.com\",'')\n",
    "                    #seg_list = jieba.cut(filecontext, cut_all=True)\n",
    "                    #corpus += seg_list\n",
    "                    count += len(filecontext)\n",
    "                    corpus.append(filecontext)\n",
    "            elif os.path.isdir(path):\n",
    "                GetData.AllFiles(self, path)\n",
    "        return corpus,count\n",
    "\n",
    "# 统计词频\n",
    "def tf(tf_dic, words):\n",
    "\n",
    "    for i in range(len(words)-1):\n",
    "        tf_dic[words[i]] = tf_dic.get(words[i], 0) + 1\n",
    "\n",
    "def bigram_term_frequency(tf_dic, words):\n",
    "    for i in range(len(words)-1):\n",
    "        tf_dic[(words[i], words[i+1])] = tf_dic.get((words[i], words[i+1]), 0) + 1\n",
    "\n",
    "def trigram_term_frequency(tf_dic, words):\n",
    "    for i in range(len(words)-2):\n",
    "        tf_dic[((words[i], words[i+1]), words[i+2])] = tf_dic.get(((words[i], words[i+1]), words[i+2]), 0) + 1\n",
    "\n",
    "def calculate_unigram(corpus,count):\n",
    "    before = time.time()\n",
    "    split_words = []\n",
    "    words_len = 0\n",
    "    line_count = 0\n",
    "    words_tf = {}\n",
    "    for line in corpus:\n",
    "        for x in jieba.cut(line):\n",
    "            split_words.append(x)\n",
    "            words_len += 1\n",
    "        tf(words_tf, split_words)\n",
    "        split_words = []\n",
    "        line_count += 1\n",
    "\n",
    "    print(\"语料库字数:\", count)\n",
    "    print(\"分词个数:\", words_len)\n",
    "    print(\"平均词长:\", round(count / words_len, 5))\n",
    "    entropy = []\n",
    "    for uni_word in words_tf.items():\n",
    "        entropy.append(-(uni_word[1] / words_len) * math.log(uni_word[1] / words_len, 2))\n",
    "    print(\"基于词的一元模型的中文信息熵为:\", round(sum(entropy), 5), \"比特/词\")\n",
    "    after = time.time()\n",
    "    print(\"运行时间:\", round(after - before, 5), \"秒\")\n",
    "\n",
    "def calculate_bigram(corpus, count):\n",
    "    before = time.time()\n",
    "    split_words = []\n",
    "    words_len = 0\n",
    "    line_count = 0\n",
    "    words_tf = {}\n",
    "    bigram_tf = {}\n",
    "\n",
    "    for line in corpus:\n",
    "        for x in jieba.cut(line):\n",
    "            split_words.append(x)\n",
    "            words_len += 1\n",
    "\n",
    "        tf(words_tf, split_words)\n",
    "        bigram_term_frequency(bigram_tf, split_words)\n",
    "\n",
    "        split_words = []\n",
    "        line_count += 1\n",
    "\n",
    "    print(\"语料库字数:\", count)\n",
    "    print(\"分词个数:\", words_len)\n",
    "    print(\"平均词长:\", round(count / words_len, 5))\n",
    "\n",
    "    bigram_len = sum([dic[1] for dic in bigram_tf.items()])\n",
    "    print(\"二元模型长度:\", bigram_len)\n",
    "\n",
    "    entropy = []\n",
    "    for bi_word in bigram_tf.items():\n",
    "        jp_xy = bi_word[1] / bigram_len  # 计算联合概率p(x,y)\n",
    "        cp_xy = bi_word[1] / words_tf[bi_word[0][0]]  # 计算条件概率p(x|y)\n",
    "        entropy.append(-jp_xy * math.log(cp_xy, 2))  # 计算二元模型的信息熵\n",
    "    print(\"基于词的二元模型的中文信息熵为:\", round(sum(entropy), 5), \"比特/词\")\n",
    "\n",
    "    after = time.time()\n",
    "    print(\"运行时间:\", round((after - before), 5), \"秒\")\n",
    "\n",
    "def calculate_trigram(corpus,count):\n",
    "    before = time.time()\n",
    "    split_words = []\n",
    "    words_len = 0\n",
    "    line_count = 0\n",
    "    words_tf = {}\n",
    "    trigram_tf = {}\n",
    "\n",
    "    for line in corpus:\n",
    "        for x in jieba.cut(line):\n",
    "            split_words.append(x)\n",
    "            words_len += 1\n",
    "\n",
    "        bigram_term_frequency(words_tf, split_words)\n",
    "        trigram_term_frequency(trigram_tf, split_words)\n",
    "\n",
    "        split_words = []\n",
    "        line_count += 1\n",
    "\n",
    "    print(\"语料库字数:\", count)\n",
    "    print(\"分词个数:\", words_len)\n",
    "    print(\"平均词长:\", round(count / words_len, 5))\n",
    "\n",
    "    trigram_len = sum([dic[1] for dic in trigram_tf.items()])\n",
    "    print(\"三元模型长度:\", trigram_len)\n",
    "\n",
    "    entropy = []\n",
    "    for tri_word in trigram_tf.items():\n",
    "        jp_xy = tri_word[1] / trigram_len  # 计算联合概率p(x,y)\n",
    "        cp_xy = tri_word[1] / words_tf[tri_word[0][0]]  # 计算条件概率p(x|y)\n",
    "        entropy.append(-jp_xy * math.log(cp_xy, 2))  # 计算三元模型的信息熵\n",
    "    print(\"基于词的三元模型的中文信息熵为:\", round(sum(entropy), 5), \"比特/词\")\n",
    "\n",
    "    after = time.time()\n",
    "    print(\"运行时间:\", round(after - before , 10), \"秒\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "mineral-verse",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/tt/cvy32cpn5b571sl6fq224h2c0000gp/T/jieba.cache\n",
      "Loading model cost 1.338 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "语料库字数: 7420081\n",
      "分词个数: 4430767\n",
      "平均词长: 1.67467\n",
      "基于词的一元模型的中文信息熵为: 12.01312 比特/词\n",
      "运行时间: 55.05731 秒\n",
      "语料库字数: 7420081\n",
      "分词个数: 4430767\n",
      "平均词长: 1.67467\n",
      "二元模型长度: 4430751\n",
      "基于词的二元模型的中文信息熵为: 6.8915 比特/词\n",
      "运行时间: 60.03709 秒\n",
      "语料库字数: 7420081\n",
      "分词个数: 4430767\n",
      "平均词长: 1.67467\n",
      "三元模型长度: 4430735\n",
      "基于词的三元模型的中文信息熵为: 2.41661 比特/词\n",
      "运行时间: 74.365352869 秒\n"
     ]
    }
   ],
   "source": [
    "data = GetData(\"/Users/smy/Desktop/HW01/data/\")\n",
    "corpus,count = data.getCorpus()#ergodic()\n",
    "calculate_unigram(corpus, count)\n",
    "calculate_bigram(corpus,count)\n",
    "calculate_trigram(corpus,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-month",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
